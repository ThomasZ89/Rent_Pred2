{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import collections\n",
    "import time\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "import glob\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "\n",
    "\n",
    "def get_text_from_clean(text, liste, direction= \"right\"):\n",
    "\n",
    "    pairs = []\n",
    "   \n",
    "    if direction == \"right\":\n",
    "        for item in liste:\n",
    "            try:\n",
    "                if item in text:\n",
    "                    pairs.append([item, text.split(item)[1].split()[0]])\n",
    "                else:\n",
    "                    pairs.append([item, \"none\"])\n",
    "            except:\n",
    "                pairs.append([item, \"none\"])\n",
    "    if direction == \"left\":\n",
    "        for item in liste:\n",
    "            try:\n",
    "                if item in text:\n",
    "                    pairs.append([item, text.split(item)[0].split()[-1]])\n",
    "                else:\n",
    "                     pairs.append([item, \"none\"])   \n",
    "            except:\n",
    "                pairs.append([item, \"none\"])\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def clean_string(liste):\n",
    "    liste = Flatten(liste)\n",
    "    liste = \" \".join(liste)\n",
    "    liste = \" \".join(liste.split())\n",
    "    return liste\n",
    "\n",
    "def get_text_from_html(bs, class_name):\n",
    "    string_list =[]\n",
    "    soup = bs.find_all(class_=class_name)\n",
    "    for entry in soup:\n",
    "        string_list.append(entry.text)\n",
    "    return string_list\n",
    "\n",
    "def Flatten(ul):\n",
    "    fl = []\n",
    "    for i in ul:\n",
    "        if type(i) is list:\n",
    "            fl += Flatten(i)\n",
    "        else:\n",
    "            fl += [i]\n",
    "    return fl\n",
    "\n",
    "def get_all_data_from_site(bs, link):\n",
    "      \n",
    "    names = [\"Wohnung\",\"Zimmergröße\",\"Sonstige\",\"Nebenkosten\",\"Miete\",\"Gesamtmiete\",\"Kaution\",\"Ablösevereinbarung\"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-12 hidden-xs\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict1 = dict(get_text_from_clean(my_list,names,\"left\"))\n",
    "       \n",
    "    names = [\"frei ab: \", \"frei bis: \"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-3\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict2 = dict(get_text_from_clean(my_list,names,\"right\"))\n",
    "    \n",
    "    names = [\" Zimmer in \"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-6\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict3 = dict(get_text_from_clean(my_list,names,\"right\"))\n",
    "       \n",
    "    names = [\"Frau \",\"Frauen \", \"Mann)\", \" Männer)\", \" Männer und\"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-6\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict4 = dict(get_text_from_clean(my_list,names,\"left\"))\n",
    "    \n",
    "    my_list = get_text_from_html(bs,\"ul-detailed-view-datasheet print_text_left\")\n",
    "    my_list = [x.strip() for x in my_list]\n",
    "    try:\n",
    "        dict5 = dict(get_text_from_clean(my_list[1],[\"zwischen\"],\"left\"))\n",
    "    except:\n",
    "        dict5 = dict(get_text_from_clean(my_list,[\"zwischen\"],\"left\"))\n",
    "        \n",
    "                                     \n",
    "    my_list = get_text_from_html(bs,\"ul-detailed-view-datasheet print_text_left\")\n",
    "    my_list = [x.strip() for x in my_list]\n",
    "    try:\n",
    "        dict8 = dict(get_text_from_clean(my_list[1],[\"Geschlecht\"],\"right\"))\n",
    "    except:\n",
    "        dict8 = dict(get_text_from_clean(my_list,[\"Geschlecht\"],\"right\"))\n",
    "\n",
    "    \n",
    "    item_list = [\"glyphicons glyphicons-bath-bathtub noprint\",\n",
    "    \"glyphicons glyphicons-wifi-alt noprint\",\n",
    "    \"glyphicons glyphicons-car noprint\",\n",
    "    \"glyphicons glyphicons-fabric noprint\",\n",
    "    \"glyphicons glyphicons-display noprint\",\n",
    "    \"glyphicons glyphicons-folder-closed noprint\",\n",
    "    \"glyphicons glyphicons-mixed-buildings noprint\",\n",
    "    \"glyphicons glyphicons-building noprint\",\n",
    "    \"glyphicons glyphicons-bus noprint\",\n",
    "    \"glyphicons glyphicons-electricity noprint\",\n",
    "    \"glyphicons glyphicons-fire noprint\"]\n",
    "    data_list = []\n",
    "    for item in item_list:\n",
    "        try:\n",
    "            data_list.append([item[22:-8], clean_string([bs.find(class_=item).next_sibling.next_sibling.next_sibling])])\n",
    "        except:\n",
    "            data_list.append([item[22:-8],\"none\"])\n",
    "    dict6 = dict(data_list)\n",
    "    \n",
    "    liste= get_text_from_html(bs,\"col-sm-4 mb10\")\n",
    "    adress_string = clean_string(liste).replace(\"Adresse \",\"\").replace(\"Umzugsfirma beauftragen1\",\"\").replace(\"Umzugsfirma beauftragen 1\",\"\")\n",
    "    dict7 = {\"Adresse\":adress_string, \"Link\": link}\n",
    "    \n",
    "    dict_list =[dict1,dict2,dict3,dict4,dict5,dict8, dict6,dict7,dict7]\n",
    "    for item in dict_list:\n",
    "        dict1.update(item)\n",
    "    return dict1\n",
    "\n",
    "def get_bs_from_html(html):\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "def get_bs_from_http(link):\n",
    "    html = requests.get(link)\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "def get_html_request(link):\n",
    "    return requests.get(link)\n",
    "\n",
    "def get_html_from_scraper(link):\n",
    "    payload = {'api_key': '5002b3eb47039688a731795808c619fd', 'url':link}\n",
    "    html = requests.get('http://api.scraperapi.com', params=payload)\n",
    "    return html\n",
    "\n",
    "def get_bs_from_http_scraper(link):\n",
    "    payload = {'api_key': '318ea492c3446fdf31906e1c267cd865', 'url':link}\n",
    "    html = requests.get('http://api.scraperapi.com', params=payload)\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def get_all_links_from_site(bs):\n",
    "    try:\n",
    "        all_links = []\n",
    "        classes = [\"listenansicht1 listenansicht-inactive offer_list_item\",\"listenansicht0 listenansicht-inactive offer_list_item\",\n",
    "                  \"listenansicht0 offer_list_item\", \"listenansicht1 offer_list_item\"]\n",
    "        \n",
    "        for scrape_class in classes:\n",
    "            links = bs.findAll(class_=scrape_class)\n",
    "            for link in links:\n",
    "                all_links.append(link[\"adid\"][31:])\n",
    "    except:\n",
    "        print(\"something went wrong with get_all_links\")\n",
    "    return all_links\n",
    "\n",
    "def get_all_links(nr_min_sites = 0, nr_max_sites = 0, sleep_time =0, scraper = get_bs_from_http_scraper):\n",
    "    linklist = []\n",
    "    for i in range(nr_min_sites,nr_max_sites):\n",
    "        try:\n",
    "            url = 'https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main.41.0.0.'+ str(i) +'.html'\n",
    "            linklist.extend(get_all_links_from_site(scraper(url)))\n",
    "            print(f\"{i+1-nr_min_sites} from {nr_max_sites-nr_min_sites} Pages loaded. Thats {(i-nr_min_sites+1)/(nr_max_sites-nr_min_sites):.2%}.\\\n",
    "            Linklist now has {len(linklist)} rows (expexted {(i+1-nr_min_sites)*20})\", end='\\r')        \n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            pass\n",
    "    return linklist   \n",
    "\n",
    "def merge_dicts(dic1,dic2):\n",
    "    try:\n",
    "        dic3 = dict(dic2)\n",
    "        for k, v in dic1.items():\n",
    "            dic3[k] = Flatten([dic3[k], v]) if k in dic3 else v\n",
    "        return dic3\n",
    "    except:\n",
    "        return dic1\n",
    "\n",
    "def csv_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            yield filename\n",
    "\n",
    "def zip_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.zip'):\n",
    "            yield filename\n",
    "\n",
    "def get_bot_and_outdated_links():\n",
    "    deleted_txt = \"Vermutlich ist das Objekt bereits vergeben.\"\n",
    "    cap_txt =\"dass kein Bot die Website automatisiert aufruft.\"\n",
    "\n",
    "    source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    theFiles = list(os.listdir(source_dir))\n",
    "\n",
    "    file_list_del = []\n",
    "    file_list_bot =[]\n",
    "    theDict = dict()\n",
    "    for i in theFiles: #Calculate size for all files here. \n",
    "        theStats = os.stat(source_dir + \"\\\\\"+i)\n",
    "        theDict[i] = theStats\n",
    "        if theDict[i].st_size < 85000 and theDict[i].st_size > 60000:\n",
    "            f = source_dir + \"\\\\\"+ i\n",
    "            with open(f, \"r\") as file:\n",
    "                a = file.read()\n",
    "                if (deleted_txt in a):\n",
    "                    file_list_del.append(i[:-4])\n",
    "                if (cap_txt in a):\n",
    "                    file_list_bot.append(i[:-4])\n",
    "\n",
    "    for item in file_list_del:\n",
    "        f = source_dir + \"\\\\\"+ item + \".txt\"\n",
    "        os.remove(f)\n",
    "\n",
    "    for item in file_list_bot:\n",
    "        f = source_dir + \"\\\\\"+ item + \".txt\"\n",
    "        os.remove(f)    \n",
    "\n",
    "    t1 = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv')\n",
    "    t2 = pd.DataFrame(file_list_del, columns =['Links'])\n",
    "    t1 = t1.append(t2).drop_duplicates(keep=\"first\")\n",
    "    t1.to_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv', index=False)\n",
    "\n",
    "    t1 = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Bot_links.csv')\n",
    "    t2 = pd.DataFrame(file_list_bot, columns =['Links'])\n",
    "    t1 = t1.append(t2).drop_duplicates(keep=\"first\")\n",
    "    t1.to_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Bot_links.csv', index=False)\n",
    "\n",
    "def zip_my_files():\n",
    "    source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    dest_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    os.chdir(dest_dir)  # To work around zipfile limitations\n",
    "    for csv_filename in csv_files(source_dir):\n",
    "        file_root = os.path.splitext(csv_filename)[0]\n",
    "        zip_file_name = file_root + '.zip'\n",
    "        zip_file_path = os.path.join(dest_dir, zip_file_name)\n",
    "        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.write(csv_filename)\n",
    "        os.remove(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files\\\\' + csv_filename)\n",
    "        \n",
    "def get_housing_links():\n",
    "    \n",
    "    get_bot_and_outdated_links()\n",
    "    zipped_files = []\n",
    "    for csv_filename in zip_files(source_dir):\n",
    "        zipped_files.append(csv_filename[:-4])\n",
    "    zipped_files = pd.DataFrame(zipped_files, columns =['Links'])\n",
    "    zipped_files[\"Links\"] = zipped_files[\"Links\"].str.lower()\n",
    "\n",
    "    house_links = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\All_Housing_Links.csv')\n",
    "    house_links[\"Links\"] = house_links[\"Links\"].str.lower()\n",
    "    outdated_links = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv')\n",
    "    outdated_links[\"Links\"] = outdated_links[\"Links\"].str.lower()\n",
    "    \n",
    "    house_links[\"Links\"] = house_links[\"Links\"].str.slice(0,-5)\n",
    "    house_links = house_links[\"Links\"].append(zipped_files[\"Links\"])\n",
    "    house_links = house_links.append(outdated_links[\"Links\"])\n",
    "    \n",
    "    house_links = house_links.drop_duplicates(keep=False)\n",
    "    return house_links            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial_links = pd.DataFrame(new_links, columns=[\"Links\"])\n",
    "#initial_links.to_csv (r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\All_Housing_Links.csv', index = None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 from 1 Pages loaded. Thats 100.00%.            Linklist now has 20 rows (expexted 20)\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Westend-Nord.7697498.html',\n",
       " 'Innenstadt.7692275.html',\n",
       " 'Nordend-West.7418667.html',\n",
       " 'Frankfurt-am-Main.4331973.html',\n",
       " 'Frankfurt-am-Main.5892499.html',\n",
       " 'Nordend-West.1829682.html',\n",
       " 'Westend-Nord.7695069.html',\n",
       " 'Dornbusch.6832992.html',\n",
       " 'Westend-Nord.7693038.html',\n",
       " 'Nordend-West.1098997.html',\n",
       " 'Westend-Nord.7695351.html',\n",
       " 'Westend-Nord.7952174.html',\n",
       " 'Westend-Nord.7743282.html',\n",
       " 'Frankfurt.4644370.html',\n",
       " 'Westend-Sud.7692756.html',\n",
       " 'Westend-Nord.7700915.html',\n",
       " 'Westend-Nord.8048404.html',\n",
       " 'Dornbusch.6354740.html',\n",
       " 'Ostend.6206905.html',\n",
       " 'Gallus.4744790.html']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_links(nr_min_sites = 0,nr_max_sites= 1, sleep_time = 0, scraper= get_bs_from_http_scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 from 30 Pages loaded. Thats 100.00%.            Linklist now has 600 rows (expexted 600)\r"
     ]
    }
   ],
   "source": [
    "##### Load old links\n",
    "old_links = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\All_Housing_Links.csv', usecols=[\"Links\"])\n",
    "##### Scrape new links\n",
    "new_links = get_all_links(nr_min_sites = 0,nr_max_sites= 30, sleep_time = 1, scraper= get_bs_from_http)\n",
    "new_links = pd.DataFrame(new_links, columns=[\"Links\"])\n",
    "#### Combine and save\n",
    "combined = pd.concat([old_links, new_links]).drop_duplicates(keep=\"first\")\n",
    "combined.to_csv (r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\All_Housing_Links.csv', index = True, header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14772\n",
      "600\n",
      "15004\n"
     ]
    }
   ],
   "source": [
    "print(len(old_links))\n",
    "print(len(new_links))\n",
    "print(len(combined))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_links = get_housing_links()\n",
    "zip_my_files()\n",
    "cap_txt =\"Alle Webseiten nutzen daher Captchas, um zu prüfen, ob beispielsweise ein Formular\"\n",
    "cap_txt2 = \"Vermutlich ist das Objekt bereits vergeben.\"\n",
    "cap_txt3 =\"Wenn Sie glauben, dass dies ein Fehler im System ist, dann schreiben Sie doch bitte eine kurze Nachricht\"\n",
    "\n",
    "for i in range(0,4498):\n",
    "    if len(house_links) < 1:\n",
    "        break\n",
    "    name = house_links.iloc[i]\n",
    "    stem = \"https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main-\"\n",
    "    link =  stem + name + \".html\"\n",
    "    path = r\"C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files\\\\\"+name + \".txt\"\n",
    "    time.sleep(120)\n",
    "    try:\n",
    "        r = get_html_request(link)\n",
    "        if cap_txt in r.text:\n",
    "            print(\"bot\",  end='\\r')\n",
    "        elif cap_txt2 in r.text:\n",
    "            print(\"vergeben\",  end='\\r')\n",
    "        elif cap_txt3 in r.text:\n",
    "            print(\"nicht gefunden\",  end='\\r')\n",
    "        else:\n",
    "            print(link,  end='\\r')\n",
    "        with open(path, 'w', encoding=\"utf-8\") as file:\n",
    "            file.write(r.text) \n",
    "    except:\n",
    "        print(\"error\",  end='\\r')\n",
    "    \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = \"https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main-Innenstadt.7692275.html\"\n",
    "bs = get_bs_from_http(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = get_all_data_from_site(bs, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['wohnung', 'zimmergröße', 'sonstige', 'nebenkosten', 'miete', 'gesamtmiete', 'kaution',\n",
    "     'ablösevereinbarung', 'frei_ab', 'frei_bis', 'personen', 'männlich', 'weiblich', 'insgesamt', 'geschlecht', 'geschlecht2', 'bath-bathtub', 'wifi-alt', 'car',\n",
    "     'fabric', 'display', 'folder-closed', 'buildings', 'stock', 'bus', 'bed',\n",
    "     'fire', 'adresse', 'link', 'taeglich', 'baujahr','rauchen','zweck_wg','keine_zweck_wg','beruf_wg','gemischt_wg','studenten_wg','frauen_wg', 'azubi_wg', 'zwischenmiete', 'renoviert', 'gender',\n",
    "           'neuwertig', 'neu_eingerichtet', 'praktikant', 'frisch_gestrichen',\n",
    "           'studentenverbindung', 'wohnheim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Wohnung', '90m²'), ('Zimmergröße', '25m²'), ('Sonstige', 'n.a.'), ('Nebenkosten', '100€'), ('Miete', '700€'), ('Gesamtmiete', '800€'), ('Kaution', 'n.a.'), ('Ablösevereinbarung', 'n.a.'), ('frei ab: ', '01.09.2020'), ('frei bis: ', 'none'), (' Zimmer in ', '3er'), ('Frau ', '1'), ('Frauen ', 'none'), ('Mann)', 'none'), (' Männer)', 'none'), (' Männer und', 'none'), ('zwischen', 'none'), ('Geschlecht', 'egal'), ('bath-bathtub', 'Dusche'), ('wifi-alt', 'Flatrate, WLAN schneller als 100 Mbit/s'), ('car', 'gute Parkmöglichkeiten'), ('fabric', 'Parkett'), ('display', 'Kabel'), ('folder-closed', 'Waschmaschine, Spülmaschine, Terrasse, Balkon, Keller, Fahrradkeller, Aufzug'), ('mixed-buildings', 'none'), ('building', 'none'), ('bus', 'none'), ('electricity', 'none'), ('fire', 'none'), ('Adresse', 'Weserstraße 7 60329 Frankfurt am Main Innenstadt '), ('Link', 'https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main-Innenstadt.7692275.html')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.DataFrame([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = [13.013,12.65,8.76,6.719,6.304,5.811,4.362,4.123,3.352,3.066,2.785,2.508,2.479,2.452,2.356,2.083,1.858,1.491,1.197,1.159,1.1,1.041,0.99,0.838,0.8,0.762,0.654,0.619,0.571,0.517,0.515,0.504,0.454,0.414,0.402,0.38,0.308,0.221,0.132,0.074,0.066,0.054,0.054,0.003]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val2 = []\n",
    "for i in val:\n",
    "    val2.append(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test',\n",
       " 'test']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
