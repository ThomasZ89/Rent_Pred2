{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import NavigableString\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "import time\n",
    "import os\n",
    "import sys\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "import glob\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.extra.rate_limiter import RateLimiter\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [20, 20]\n",
    "import seaborn as sns\n",
    "\n",
    "from pylab import rcParams\n",
    "\n",
    "import catboost\n",
    "from catboost import CatBoostRegressor\n",
    "from catboost import Pool, cv\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option(\"max_rows\", 20)\n",
    "#pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "api_key = '5002b3eb47039688a731795808c619fd'\n",
    "\n",
    "def csv_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            yield filename\n",
    "\n",
    "def zip_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.zip'):\n",
    "            yield filename            \n",
    "\n",
    "\n",
    "def get_bs_from_html(html):\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "\n",
    "\n",
    "def get_text_from_clean(text, liste, direction= \"right\"):\n",
    "\n",
    "    pairs = []\n",
    "   \n",
    "    if direction == \"right\":\n",
    "        for item in liste:\n",
    "            try:\n",
    "                if item in text:\n",
    "                    pairs.append([item, text.split(item)[1].split()[0]])\n",
    "                else:\n",
    "                    pairs.append([item, \"none\"])\n",
    "            except:\n",
    "                pairs.append([item, \"none\"])\n",
    "    if direction == \"left\":\n",
    "        for item in liste:\n",
    "            try:\n",
    "                if item in text:\n",
    "                    pairs.append([item, text.split(item)[0].split()[-1]])\n",
    "                else:\n",
    "                     pairs.append([item, \"none\"])   \n",
    "            except:\n",
    "                pairs.append([item, \"none\"])\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def clean_string(liste):\n",
    "    liste = Flatten(liste)\n",
    "    liste = \" \".join(liste)\n",
    "    liste = \" \".join(liste.split())\n",
    "    return liste\n",
    "\n",
    "def get_text_from_html(bs, class_name):\n",
    "    string_list =[]\n",
    "    soup = bs.find_all(class_=class_name)\n",
    "    for entry in soup:\n",
    "        string_list.append(entry.text)\n",
    "    return string_list\n",
    "\n",
    "def Flatten(ul):\n",
    "    fl = []\n",
    "    for i in ul:\n",
    "        if type(i) is list:\n",
    "            fl += Flatten(i)\n",
    "        else:\n",
    "            fl += [i]\n",
    "    return fl\n",
    "\n",
    "def get_all_data_from_site(bs, link):\n",
    "      \n",
    "    names = [\"Wohnung\",\"Zimmergröße\",\"Sonstige\",\"Nebenkosten\",\"Miete\",\"Gesamtmiete\",\"Kaution\",\"Ablösevereinbarung\"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-12 hidden-xs\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict1 = dict(get_text_from_clean(my_list,names,\"left\"))\n",
    "       \n",
    "    names = [\"frei ab: \", \"frei bis: \"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-3\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict2 = dict(get_text_from_clean(my_list,names,\"right\"))\n",
    "    \n",
    "    names = [\" Zimmer in \"]\n",
    "    my_list = get_text_from_html(bs,\"col-sm-6\")\n",
    "    my_list = clean_string(my_list)\n",
    "    dict3 = dict(get_text_from_clean(my_list,names,\"right\"))\n",
    "       \n",
    "    \n",
    "    names = [\"Malmännliche\",\"weiblich\",'height=\"17\"']\n",
    "    count = []\n",
    "    for name in names:\n",
    "        try:\n",
    "            string = str(bs.find(class_=\"mr5 detail-list-fav-button display-inline-block hidden-xs create_favourite\").next_sibling.next_sibling)\n",
    "            count.append(string.count(name))\n",
    "        except:\n",
    "            count.append(\"none\")\n",
    "    dict4 = dict(zip(names, count))\n",
    "      \n",
    "    \n",
    "    my_list = get_text_from_html(bs,\"ul-detailed-view-datasheet print_text_left\")\n",
    "    my_list = [x.strip() for x in my_list]\n",
    "    try:\n",
    "        dict5 = dict(get_text_from_clean(my_list[1],[\"zwischen\"],\"left\"))\n",
    "    except:\n",
    "        dict5 = dict(get_text_from_clean(my_list,[\"zwischen\"],\"left\"))\n",
    "        \n",
    "                                     \n",
    "    my_list = get_text_from_html(bs,\"ul-detailed-view-datasheet print_text_left\")\n",
    "    my_list = [x.strip() for x in my_list]\n",
    "    try:\n",
    "        dict8 = dict(get_text_from_clean(my_list[1],[\"Geschlecht\"],\"right\"))\n",
    "    except:\n",
    "        dict8 = dict(get_text_from_clean(my_list,[\"Geschlecht\"],\"right\"))\n",
    "\n",
    "    \n",
    "    item_list = [\"glyphicons glyphicons-bath-bathtub noprint\",\n",
    "    \"glyphicons glyphicons-wifi-alt noprint\",\n",
    "    \"glyphicons glyphicons-car noprint\",\n",
    "    \"glyphicons glyphicons-fabric noprint\",\n",
    "    \"glyphicons glyphicons-display noprint\",\n",
    "    \"glyphicons glyphicons-folder-closed noprint\",\n",
    "    \"glyphicons glyphicons-mixed-buildings noprint\",\n",
    "    \"glyphicons glyphicons-building noprint\",\n",
    "    \"glyphicons glyphicons-bus noprint\",\n",
    "    \"glyphicons glyphicons-bed noprint\",\n",
    "    \"glyphicons glyphicons-fire noprint\"]\n",
    "    data_list = []\n",
    "    for item in item_list:\n",
    "        try:\n",
    "            data_list.append([item[22:-8], clean_string([bs.find(class_=item).next_sibling.next_sibling.next_sibling])])\n",
    "        except:\n",
    "            data_list.append([item[22:-8],\"none\"])\n",
    "    dict6 = dict(data_list)\n",
    "    \n",
    "    liste= get_text_from_html(bs,\"col-sm-4 mb10\")\n",
    "    adress_string = clean_string(liste).replace(\"Adresse \",\"\").replace(\"Umzugsfirma beauftragen1\",\"\").replace(\"Umzugsfirma beauftragen 1\",\"\")\n",
    "    dict7 = {\"Adresse\":adress_string, \"Link\": link}\n",
    "    \n",
    "    names = \"Miete pro Tag: \"\n",
    "    my_list = get_text_from_html(bs,\"col-sm-5\")\n",
    "    my_list = clean_string(my_list)\n",
    "    if names in my_list:\n",
    "        dict9 = {\"taeglich\":1}\n",
    "    else:\n",
    "        dict9 = {\"taeglich\":0}\n",
    "    \n",
    "    div_id = 'popover-energy-certification'\n",
    "    try:\n",
    "        cs = clean_string([bs.find(id=div_id).next_sibling])\n",
    "        dict10 = {\"baujahr\" : cs}\n",
    "    except:\n",
    "        dict10 = {\"baujahr\" : \"none\"}\n",
    "    \n",
    "    rauchen= \"Rauchen nicht erwünscht\"\n",
    "    nichrauchen = \"Rauchen überall erlaubt\"\n",
    "    my_list = get_text_from_html(bs,\"col-sm-6\")\n",
    "    my_list = clean_string(my_list)\n",
    "    if rauchen in my_list:\n",
    "        dict11 = {\"rauchen\":\"raucher\"}\n",
    "    if nichrauchen in my_list:\n",
    "        dict11 = {\"rauchen\":\"nichtraucher\"}\n",
    "    if rauchen not in my_list and nichrauchen not in my_list:\n",
    "        dict11 = {\"rauchen\":\"keine_Angabe\"}\n",
    "    \n",
    "    wg_list = [\"Zweck-WG\",\"keine Zweck-WG\",\"Berufstätigen-WG\", \"gemischte WG\",\"Studenten-WG\",\"Frauen-WG\",\"Azubi-WG\"]\n",
    "    dict12 = []\n",
    "    for wg in wg_list:\n",
    "        my_list = get_text_from_html(bs,\"col-sm-6\")\n",
    "        my_list = clean_string(my_list)\n",
    "        if wg in my_list:\n",
    "            dict12.append([wg,1])\n",
    "        else:\n",
    "            dict12.append([wg,0])\n",
    "    dict12 = dict(dict12)\n",
    "\n",
    "    dict_list =[dict1,dict2,dict3,dict4,dict5,dict8, dict6,dict7,dict7,dict9,dict10, dict11, dict12]\n",
    "    for item in dict_list:\n",
    "        dict1.update(item)\n",
    "    return dict1\n",
    "\n",
    "def get_bs_from_html(html):\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "def get_bs_from_http(link):\n",
    "    html = requests.get(link)\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "def get_html_request(link):\n",
    "    return requests.get(link)\n",
    "\n",
    "def get_html_from_scraper(link):\n",
    "    payload = {'api_key': api_key, 'url':link}\n",
    "    html = requests.get('http://api.scraperapi.com', params=payload)\n",
    "    return html\n",
    "\n",
    "def get_bs_from_http_scraper(link):\n",
    "    payload = {'api_key':  api_key, 'url':link}\n",
    "    html = requests.get('http://api.scraperapi.com', params=payload)\n",
    "    return BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "\n",
    "def get_all_links_from_site(bs):\n",
    "    try:\n",
    "        all_links = []\n",
    "        classes = [\"listenansicht1 listenansicht-inactive offer_list_item\",\"listenansicht0 listenansicht-inactive offer_list_item\",\n",
    "                  \"listenansicht0 offer_list_item\", \"listenansicht1 offer_list_item\"]\n",
    "        \n",
    "        for scrape_class in classes:\n",
    "            links = bs.findAll(class_=scrape_class)\n",
    "            for link in links:\n",
    "                all_links.append(link[\"adid\"][31:])\n",
    "    except:\n",
    "        print(\"something went wrong with get_all_links\")\n",
    "    return all_links\n",
    "\n",
    "def get_all_links(nr_min_sites = 0, nr_max_sites = 0, sleep_time =0):\n",
    "    linklist = []\n",
    "    for i in range(nr_min_sites,nr_max_sites):\n",
    "        try:\n",
    "            url = 'https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main.41.0.0.'+ str(i) +'.html'\n",
    "            linklist.extend(get_all_links_from_site(get_bs_from_http_scraper(url)))\n",
    "            print(f\"{i+1-nr_min_sites} from {nr_max_sites-nr_min_sites} Pages loaded. Thats {(i-nr_min_sites+1)/(nr_max_sites-nr_min_sites):.2%}.\\\n",
    "            Linklist now has {len(linklist)} rows (expexted {(i+1-nr_min_sites)*20})\", end='\\r')        \n",
    "            time.sleep(sleep_time)\n",
    "        except:\n",
    "            pass\n",
    "    return linklist   \n",
    "\n",
    "def merge_dicts(dic1,dic2):\n",
    "    try:\n",
    "        dic3 = dict(dic2)\n",
    "        for k, v in dic1.items():\n",
    "            dic3[k] = Flatten([dic3[k], v]) if k in dic3 else v\n",
    "        return dic3\n",
    "    except:\n",
    "        return dic1\n",
    "\n",
    "def csv_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            yield filename\n",
    "\n",
    "def zip_files(source_dir):\n",
    "    for filename in os.listdir(source_dir):\n",
    "        if filename.endswith('.zip'):\n",
    "            yield filename\n",
    "\n",
    "def get_bot_and_outdated_links():\n",
    "    deleted_txt = \"Vermutlich ist das Objekt bereits vergeben.\"\n",
    "    cap_txt =\"dass kein Bot die Website automatisiert aufruft.\"\n",
    "\n",
    "    source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    theFiles = list(os.listdir(source_dir))\n",
    "\n",
    "    file_list_del = []\n",
    "    file_list_bot =[]\n",
    "    theDict = dict()\n",
    "    for i in theFiles: #Calculate size for all files here. \n",
    "        theStats = os.stat(source_dir + \"\\\\\"+i)\n",
    "        theDict[i] = theStats\n",
    "        if theDict[i].st_size < 85000 and theDict[i].st_size > 60000:\n",
    "            f = source_dir + \"\\\\\"+ i\n",
    "            with open(f, \"r\") as file:\n",
    "                a = file.read()\n",
    "                if (deleted_txt in a):\n",
    "                    file_list_del.append(i[:-4])\n",
    "                if (cap_txt in a):\n",
    "                    file_list_bot.append(i[:-4])\n",
    "\n",
    "    for item in file_list_del:\n",
    "        f = source_dir + \"\\\\\"+ item + \".txt\"\n",
    "        os.remove(f)\n",
    "\n",
    "    for item in file_list_bot:\n",
    "        f = source_dir + \"\\\\\"+ item + \".txt\"\n",
    "        os.remove(f)    \n",
    "\n",
    "    t1 = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv')\n",
    "    t2 = pd.DataFrame(file_list_del, columns =['Links'])\n",
    "    t1 = t1.append(t2).drop_duplicates(keep=\"first\")\n",
    "    t1.to_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv', index=False)\n",
    "\n",
    "    t1 = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Bot_links.csv')\n",
    "    t2 = pd.DataFrame(file_list_bot, columns =['Links'])\n",
    "    t1 = t1.append(t2).drop_duplicates(keep=\"first\")\n",
    "    t1.to_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Bot_links.csv', index=False)\n",
    "\n",
    "def zip_my_files():\n",
    "    source_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    dest_dir = r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files'\n",
    "    os.chdir(dest_dir)  # To work around zipfile limitations\n",
    "    for csv_filename in csv_files(source_dir):\n",
    "        file_root = os.path.splitext(csv_filename)[0]\n",
    "        zip_file_name = file_root + '.zip'\n",
    "        zip_file_path = os.path.join(dest_dir, zip_file_name)\n",
    "        with zipfile.ZipFile(zip_file_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "            zf.write(csv_filename)\n",
    "        os.remove(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Files\\\\' + csv_filename)\n",
    "        \n",
    "def get_housing_links():\n",
    "    \n",
    "    get_bot_and_outdated_links()\n",
    "    zipped_files = []\n",
    "    for csv_filename in zip_files(source_dir):\n",
    "        zipped_files.append(csv_filename[:-4])\n",
    "    zipped_files = pd.DataFrame(zipped_files, columns =['Links'])\n",
    "\n",
    "    house_links = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\All_Housing_Links.csv')\n",
    "    outdated_links = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\Outdated_links.csv')\n",
    "    \n",
    "    house_links[\"Links\"] = house_links[\"Links\"].str.slice(0,-5)\n",
    "    house_links = house_links[\"Links\"].append(zipped_files[\"Links\"])\n",
    "    house_links = house_links.append(outdated_links[\"Links\"])\n",
    "    \n",
    "    house_links = house_links.drop_duplicates(keep=False)\n",
    "    return house_links            \n",
    "\n",
    "def replace_viertel(x, viertel_liste):\n",
    "    if x in viertel_liste:\n",
    "        return x\n",
    "    elif any([i in x for i in viertel_liste]):\n",
    "        return [i for (i, v) in zip(viertel_liste, [i in x for i in viertel_liste]) if v][0]\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def link_to_pandas(full_link):\n",
    "    stem = full_link[:57]\n",
    "    link = full_link[57:]\n",
    "    bs = get_bs_from_http(stem+link)\n",
    "    data = get_all_data_from_site(bs, link)\n",
    "    df = pd.DataFrame([data], columns=data.keys())\n",
    "    df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "    #rename cols\n",
    "    df.columns = ['wohnung', 'zimmergröße', 'sonstige', 'nebenkosten', 'miete', 'gesamtmiete', 'kaution',\n",
    "     'ablösevereinbarung', 'frei_ab', 'frei_bis', 'personen', 'männlich', 'weiblich', 'insgesamt', 'geschlecht', 'geschlecht2', 'bath-bathtub', 'wifi-alt', 'car',\n",
    "     'fabric', 'display', 'folder-closed', 'buildings', 'stock', 'bus', 'bed',\n",
    "     'fire', 'adresse', 'link', 'taeglich', 'baujahr','rauchen','zweck_wg','keine_zweck_wg','beruf_wg','gemischt_wg','studenten_wg','frauen_wg', 'azubi_wg']\n",
    "    #remove common words from cols\n",
    "    remove_list = [\"m²\",\"€\",r\"n.a\",\"none\",\"(\",\")\", \". og\",\"minuten zu fuß entfernt\",\"minute zu fuß entfernt\"]\n",
    "    for col in list(df.columns):\n",
    "        for item in remove_list:\n",
    "            df[col] = df[col].str.replace(item,\"\", regex=False)\n",
    "\n",
    "    #remove individual words from col\n",
    "    df[\"personen\"] = df[\"personen\"].str.replace(\"er\",\"\")\n",
    "\n",
    "    df[\"straße\"]   = df.adresse.str.extract(pat=\"(.*)\\d\\d\\d\\d\\d\")\n",
    "    df[\"straße\"]   = df.straße.str.replace(\"str\\.\",\"straße\")\n",
    "    df[\"straße\"]   = df.straße.str.replace(\"str \",\"straße\")\n",
    "    df[\"straße\"]   = df.straße.str.replace(\"strasse\",\"straße\")\n",
    "\n",
    "    # Vermutlich ist möbliert, teilmöbliert = teilmöbliert\n",
    "    df[\"bed\"]      = df.bed.str.replace(\"möbliert, teilmöbliert\",\"teilmöbliert\")\n",
    "\n",
    "\n",
    "    df[\"geschlecht\"] = df[\"geschlecht\"] + df[\"geschlecht2\"]\n",
    "    df[\"geschlecht\"] = df[\"geschlecht\"].str.replace(\"egalegal\",\"egal\")\n",
    "    # drop unused cols\n",
    "    #,\"Adresse\"\n",
    "    df = df.drop(columns=[\"geschlecht2\"])\n",
    "\n",
    "    df[\"folder-closed\"] = df[\"folder-closed\"].str.replace(\"haustiere erlaubt\",\"haustiere\")\n",
    "    df[\"bath-bathtub\"] = df[\"bath-bathtub\"].str.replace(\"eigenes bad\",\"eigenes_bad\")\n",
    "    df[\"bath-bathtub\"] = df[\"bath-bathtub\"].str.replace(\"gäste wc\",\"gäste_wc\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    class_list =[\"aufzug, balkon, fahrradkeller, garten, gartenmitbenutzung, haustiere, keller, spülmaschine, terrasse, waschmaschine\",\n",
    "                  \"dielen, fliesen, fußbodenheizung, laminat, parkett, pvc, teppich\",\n",
    "                  \"badewanne, badmitbenutzung, dusche, eigenes_bad, gäste_wc\",\n",
    "                  \"kabel, satellit\"]\n",
    "    \n",
    "    df = pd.concat([df,df], ignore_index= True)\n",
    "    \n",
    "    one_hot_cols = [\"folder-closed\",'fabric','bath-bathtub',\"display\"]\n",
    "    for i, col in enumerate(one_hot_cols):\n",
    "        df.iloc[1, df.columns.get_loc(col)] = class_list[i]\n",
    "        df2 = df[col].str.get_dummies(', ')\n",
    "        df = pd.concat([df, df2.reindex(df.index)], axis=1)\n",
    "    df = df.head(1)\n",
    "    \n",
    "    df = df.set_index(\"link\")\n",
    "    \n",
    "    df = df.drop(columns= one_hot_cols)\n",
    "    df[\"viertel\"] = df.index.to_series().astype(str).str.extract(pat=\"(.*)\\.\\d\\d\\d\\d\\d\\d\")\n",
    "    df[\"viertel\"] = df[\"viertel\"].str.lower()\n",
    "    repl_viertel = [\"--\",\"\\d\\d\\d\\d\\d\",\"franfurter \",\"frankfurt am main\",\"franfurt-am-main,\"\"frankfurt-main-\",\"frankfurtnord\",\"frankfurt-\",\"frankfurt\",\"bei-frankfurt\",\"naehe\",\"sudlich-von\",\"u-bahn-station-\",\"-bei-ffm\",\"1-minute-from-\",\"am-main\" ]\n",
    "    for word in repl_viertel:\n",
    "        df[\"viertel\"] = df[\"viertel\"].str.replace(word,\"\")\n",
    "    df[\"viertel\"] = df[\"viertel\"].str.strip('-')\n",
    "\n",
    "    conditions = [\n",
    "        (df[\"frei_ab\"] == \"\"),\n",
    "        ((df[\"frei_ab\"] != \"\") & (df[\"frei_bis\"] != \"\")),\n",
    "        ((df[\"frei_ab\"] != \"\") & (df[\"frei_bis\"] == \"\"))]\n",
    "    choices = ['inaktiv', 'befristet', 'unbefristet']\n",
    "    df[\"status\"] = np.select(conditions, choices, default='black')\n",
    "\n",
    "    df[\"dauer\"] = pd.to_datetime(df.frei_bis, format='%d.%m.%Y', errors='coerce') - pd.to_datetime(df.frei_ab, format='%d.%m.%Y', errors='coerce')\n",
    "    df['dauer'] = df['dauer'] / np.timedelta64(1, 'D')\n",
    "    df['dauer'].fillna(0, inplace=True)\n",
    "\n",
    "    df[\"wohnung\"] = df[\"wohnung\"].str.replace(\"\\.\",\"\")\n",
    "    df[\"m2_pro_pers\"] = pd.to_numeric(df['wohnung'], errors='coerce')/ pd.to_numeric(df['personen'], errors='coerce')\n",
    "\n",
    "    # Replace uncommon places with common places if they are included in common place\n",
    "    # Since PLZ is more precise viertel_name will not be used for modelling\n",
    "    #df[\"viertel_name\"] = df.viertel.apply(replace_viertel, viertel_liste=freq_viertel)\n",
    "    df_mapped = df\n",
    "    df_mapped[\"baujahr\"] = df_mapped.baujahr.str.extract(pat=\"baujahr (\\d\\d\\d\\d)\")\n",
    "    df_mapped[\"PLZ\"] = df_mapped.adresse.str.extract(\"(\\d\\d\\d\\d\\d)\")\n",
    "\n",
    "\n",
    "    num_cols = ['wohnung', 'zimmergröße', 'sonstige', 'nebenkosten', 'miete', 'gesamtmiete', 'bus', 'männlich',\n",
    "     'personen', 'weiblich', 'kaution', 'ablösevereinbarung', 'personen', 'bus', 'baujahr',\"taeglich\"]\n",
    "    for col in num_cols:\n",
    "        df_mapped[col] = df_mapped[col].astype(str)\n",
    "        df_mapped[col] = df_mapped[col].str.extract('(\\d+)', expand=False)\n",
    "        df_mapped[col] = df_mapped[col].astype(float)    \n",
    "    return df    \n",
    "\n",
    "\n",
    "freq_viertel = ['sachsenhausen', 'bockenheim', 'bornheim', 'nordend-ost', 'nordend-west', 'ostend', 'innenstadt', 'niederrad', 'westend-nord', 'dornbusch', 'gallusviertel', 'gallus', 'bahnhofsviertel', 'westend-sud', 'roedelheim', 'hoechst', 'eschersheim', 'gutleutviertel', 'griesheim', 'oberrad', 'ginnheim', 'heddernheim', 'eckenheim', 'hausen', 'preungesheim', 'flughafen', 'fechenheim', 'altstadt', 'nied', 'bergen-enkheim', 'nieder-eschbach', 'bonames', 'praunheim', 'sossenheim', 'niederursel', 'nordend', 'offenbach', 'seckbach', 'berkersheim', 'kelsterbach', 'unterliederbach', 'sindlingen', 'neu-isenburg', 'schwanheim', 'westend', 'kalbach', 'er-berg', 'europaviertel', 'zeilsheim', 'harheim', 'eschborn', 'riederwald', 'riedberg', 'bad-vilbel', 'goldstein', 'raunheimflughafen', 'nieder-erlenbach', 'oberursel', 'maintal', 'raunheim', 'moerfelden-walldorf', 'kaiserlei', 'nordweststadt', 'langen', 'sachsenhausen-nord', 'eschborn-bei', 'bad-homburg', 'nordend-bornheim', 'rodgau']\n",
    "plz_list = ['60327', '60326', '65931', '60313', '60325', '60323', '63263', '60322', '63067', '60389', '60486', '65451', '60487', '60311', '60320', '60329', '60318', '65929', '65934', '61440', '63110', '60529', '60488', '60599', '60596', '60314', '61118', '65936', '65719', '60594', '60385', '60388', '65479', '65824', '60598', '60528', '60549', '60386', '65428', '60489', '60438', '60435', '60439', '60433', '60316', '63065', '63069', '60437', '60431', '64546', '65933', '63477', '63225', '61348', '65760']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data from saved htmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14789\r"
     ]
    }
   ],
   "source": [
    "data_dict ={}\n",
    "i= 0\n",
    "for file in zip_files(source_dir):\n",
    "    i+=1\n",
    "    print(i, end='\\r')\n",
    "    file_dir = source_dir + \"\\\\\" + file\n",
    "    with ZipFile(file=file_dir) as zip_file:\n",
    "        bs = zip_file.read(zip_file.namelist()[0]).decode('utf8')\n",
    "        soup = BeautifulSoup(bs, \"html.parser\")\n",
    "        new_dict = get_all_data_from_site(soup,file[:-4])\n",
    "        if len(data_dict) == 0:\n",
    "            data_dict = new_dict \n",
    "        else:\n",
    "            data_dict =merge_dicts(data_dict,new_dict)\n",
    "        if i == 999999:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14789"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(source_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Dict in DataFrame and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_dict,dtype=str)\n",
    "df = df.apply(lambda x: x.astype(str).str.lower())\n",
    "#rename cols\n",
    "df.columns = ['wohnung', 'zimmergröße', 'sonstige', 'nebenkosten', 'miete', 'gesamtmiete', 'kaution',\n",
    " 'ablösevereinbarung', 'frei_ab', 'frei_bis', 'personen', 'männlich', 'weiblich', 'insgesamt', 'geschlecht', 'geschlecht2', 'bath-bathtub', 'wifi-alt', 'car',\n",
    " 'fabric', 'display', 'folder-closed', 'buildings', 'stock', 'bus', 'bed',\n",
    " 'fire', 'adresse', 'link', 'taeglich', 'baujahr','rauchen','zweck_wg','keine_zweck_wg','beruf_wg','gemischt_wg','studenten_wg','frauen_wg', 'azubi_wg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove common words from cols\n",
    "remove_list = [\"m²\",\"€\",r\"n.a\",\"none\",\"(\",\")\", \". og\",\"minuten zu fuß entfernt\",\"minute zu fuß entfernt\"]\n",
    "for col in list(df.columns):\n",
    "    for item in remove_list:\n",
    "        df[col] = df[col].str.replace(item,\"\", regex=False)\n",
    "        \n",
    "#remove individual words from col\n",
    "df[\"personen\"] = df[\"personen\"].str.replace(\"er\",\"\")\n",
    "\n",
    "df[\"straße\"] = df.adresse.str.extract(pat=\"(.*)\\d\\d\\d\\d\\d\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"str\\.\",\"straße\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"str \",\"straße\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"strasse\",\"straße\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"[^\\w\\d]\",\"\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"[0-9]+\",\"\")\n",
    "df[\"straße\"] = df.straße.str.replace(\"ß\",\"ss\")\n",
    "df.loc[~df[\"straße\"].isin(list(pd.DataFrame(df.straße.value_counts()).query(\"straße > 10\").index)), \"straße\"] = \"no_info_or_rare\"\n",
    "\n",
    "# Vermutlich ist möbliert, teilmöbliert = teilmöbliert\n",
    "df[\"bed\"]      = df.bed.str.replace(\"möbliert, teilmöbliert\",\"teilmöbliert\")\n",
    "\n",
    "\n",
    "df[\"geschlecht\"] = df[\"geschlecht\"] + df[\"geschlecht2\"]\n",
    "df[\"geschlecht\"] = df[\"geschlecht\"].str.replace(\"egalegal\",\"egal\")\n",
    "# drop unused cols\n",
    "#,\"Adresse\"\n",
    "df = df.drop(columns=[\"geschlecht2\"])\n",
    "\n",
    "df[\"folder-closed\"] = df[\"folder-closed\"].str.replace(\"haustiere erlaubt\",\"haustiere\")\n",
    "df[\"bath-bathtub\"] = df[\"bath-bathtub\"].str.replace(\"eigenes bad\",\"eigenes_bad\")\n",
    "df[\"bath-bathtub\"] = df[\"bath-bathtub\"].str.replace(\"gäste wc\",\"gäste_wc\")\n",
    "\n",
    "\n",
    "one_hot_cols = [\"folder-closed\",'fabric','bath-bathtub',\"display\"]\n",
    "class_list = [['', 'aufzug','balkon','fahrradkeller', 'garten', 'gartenmitbenutzung','haustiere','keller','spülmaschine','terrasse','waschmaschine'],\n",
    "              ['', 'dielen','fliesen','fußbodenheizung','laminat','parkett','pvc','teppich'],\n",
    "              ['', 'badewanne','badmitbenutzung','dusche','eigenes_bad','gäste_wc'],\n",
    "              ['', 'kabel','satellit']]\n",
    "for col in one_hot_cols:\n",
    "    df[col] = df[col].str.replace(\",\",\"\")\n",
    "    df[col] = df[col].str.split(\" \").str[:]\n",
    "\n",
    "df_dict = {elem : pd.DataFrame() for elem in one_hot_cols}\n",
    "mlb = MultiLabelBinarizer()\n",
    "for i, col in enumerate(df_dict.keys()):\n",
    "    df_dict[col] = pd.DataFrame(mlb.fit_transform(df[col]),columns=class_list[i], index=df.link)\n",
    "\n",
    "df = df.set_index(\"link\")\n",
    "for col in df_dict:\n",
    "    df = df.join(df_dict[col]).drop(\"\", axis=1)\n",
    "    \n",
    "df = df.drop(columns= one_hot_cols)\n",
    "df[\"viertel\"] = df.index.to_series().astype(str).str.extract(pat=\"(.*)\\.\\d\\d\\d\\d\\d\\d\")\n",
    "df[\"viertel\"] = df[\"viertel\"].str.lower()\n",
    "repl_viertel = [\"--\",\"\\d\\d\\d\\d\\d\",\"franfurter \",\"frankfurt am main\",\"franfurt-am-main,\"\"frankfurt-main-\",\"frankfurtnord\",\"frankfurt-\",\"frankfurt\",\"bei-frankfurt\",\"naehe\",\"sudlich-von\",\"u-bahn-station-\",\"-bei-ffm\",\"1-minute-from-\",\"am-main\" ]\n",
    "for word in repl_viertel:\n",
    "    df[\"viertel\"] = df[\"viertel\"].str.replace(word,\"\")\n",
    "df[\"viertel\"] = df[\"viertel\"].str.strip('-')\n",
    "\n",
    "conditions = [\n",
    "    (df[\"frei_ab\"] == \"\"),\n",
    "    ((df[\"frei_ab\"] != \"\") & (df[\"frei_bis\"] != \"\")),\n",
    "    ((df[\"frei_ab\"] != \"\") & (df[\"frei_bis\"] == \"\"))]\n",
    "choices = ['inaktiv', 'befristet', 'unbefristet']\n",
    "df[\"status\"] = np.select(conditions, choices)\n",
    "\n",
    "df[\"direct_link\"]= \"https://www.wg-gesucht.de/wg-zimmer-in-Frankfurt-am-Main-\" + df.index + \".html\"\n",
    "\n",
    "df[\"dauer\"] = pd.to_datetime(df.frei_bis, format='%d.%m.%Y', errors='coerce') - pd.to_datetime(df.frei_ab, format='%d.%m.%Y', errors='coerce')\n",
    "df['dauer'] = df['dauer'] / np.timedelta64(1, 'D')\n",
    "df['dauer'].fillna(0, inplace=True)\n",
    "\n",
    "df[\"wohnung\"] = df[\"wohnung\"].str.replace(\"\\.\",\"\")\n",
    "df[\"m2_pro_pers\"] = pd.to_numeric(df['wohnung'], errors='coerce')/ pd.to_numeric(df['personen'], errors='coerce')\n",
    "\n",
    "# Replace uncommon places with common places if they are included in common places\n",
    "df[\"viertel_name\"] = df.viertel.apply(replace_viertel, viertel_liste=freq_viertel)\n",
    "df_mapped = df\n",
    "df_mapped[\"baujahr\"] = df_mapped.baujahr.str.extract(pat=\"baujahr (\\d\\d\\d\\d)\")\n",
    "df_mapped[\"PLZ\"] = df_mapped.adresse.str.extract(\"(\\d\\d\\d\\d\\d)\")\n",
    "\n",
    "# Replace uncommon PLZ with new value\n",
    "# df_mapped.loc[~df_mapped[\"PLZ\"].isin(list(pd.DataFrame(df.PLZ.value_counts()).query(\"PLZ > 10\").index)), \"PLZ\"] = 99999\n",
    "\n",
    "num_cols = ['wohnung', 'zimmergröße', 'sonstige', 'nebenkosten', 'miete', 'gesamtmiete', 'bus', 'männlich',\n",
    " 'personen', 'weiblich', 'kaution', 'ablösevereinbarung', 'personen', 'bus', 'baujahr',\"taeglich\"]\n",
    "for col in num_cols:\n",
    "    df_mapped[col] = df_mapped[col].astype(str)\n",
    "    df_mapped[col] = df_mapped[col].str.extract('(\\d+)', expand=False)\n",
    "    df_mapped[col] = df_mapped[col].astype(float)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_mapped.to_csv (r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\data_df.csv', index = True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter outliers\n",
    "filtered_df = df_mapped.query(\"taeglich != '1'\")\n",
    "filtered_df = filtered_df.query(\"gesamtmiete < 1200\")\n",
    "filtered_df = filtered_df.query(\"gesamtmiete > 150\")\n",
    "filtered_df = filtered_df.query(\"zimmergröße < 60\")\n",
    "filtered_df = filtered_df.query(\"zimmergröße > 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace na's\n",
    "#keine angabe\n",
    "feat_list1 = [\"buildings\", \"stock\", \"bed\", \"car\", \"fire\", \"straße\",\"PLZ\"]\n",
    "#median\n",
    "feat_list2 = [\"wohnung\",\"bus\",\"personen\",\"m2_pro_pers\"]\n",
    "# null setzen\n",
    "feat_list3 = [\"baujahr\",\"ablösevereinbarung\"]\n",
    "\n",
    "for feat in feat_list1:\n",
    "    filtered_df[feat].fillna(\"no_info_or_rare\", inplace=True)\n",
    "    filtered_df[feat] = filtered_df[feat].replace(r'^\\s*$', \"no_info_or_rare\", regex=True)\n",
    "\n",
    "for feat in feat_list2:\n",
    "    filtered_df[feat].fillna(filtered_df[feat].median(), inplace=True)\n",
    "    filtered_df[feat] = filtered_df[feat].replace(r'^\\s*$', filtered_df[feat].median(), regex=True)\n",
    "\n",
    "for feat in feat_list3:\n",
    "    filtered_df[feat].fillna(0, inplace=True)\n",
    "    filtered_df[feat] = filtered_df[feat].replace(r'^\\s*$', 0, regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_df.to_csv (r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\filtered_df.csv', index = True, header=True)\n",
    "filtered_df = pd.read_csv(r'C:\\Users\\Thomas.Zoellinger\\Documents\\Jupyter Notebooks\\Housing\\filtered_df.csv')\n",
    "filtered_df.PLZ = filtered_df.PLZ.astype(str) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
